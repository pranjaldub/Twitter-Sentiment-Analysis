{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/IMDB Dataset.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_reviews = pd.read_csv(\"/kaggle/input/IMDB Dataset.csv\")\n\nmovie_reviews.isnull().values.any()\n\nmovie_reviews.shape","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"(50000, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(sen):\n    # Removing html tags\n    sentence = remove_tags(sen)\n\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TAG_RE = re.compile(r'<[^>]+>')\n\ndef remove_tags(text):\n    return TAG_RE.sub('', text)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = []\nsentences = list(movie_reviews['review'])\nfor sen in sentences:\n    X.append(preprocess_text(sen))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = movie_reviews['sentiment']\n\ny = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"movie_reviews = pd.read_csv(\"/kaggle/input/IMDB Dataset.csv\")\n\nmovie_reviews.isnull().values.any()\n\nmovie_reviews.shape"},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_reviews.head()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_reviews[\"review\"][10]","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"'Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines.<br /><br />At first it was very odd and pretty funny but as the movie progressed I didn\\'t find the jokes or oddness funny anymore.<br /><br />Its a low budget film (thats never a problem in itself), there were some pretty interesting characters, but eventually I just lost interest.<br /><br />I imagine this film would appeal to a stoner who is currently partaking.<br /><br />For something similar but better try \"Brother from another planet\"'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nsns.countplot(x='sentiment', data=movie_reviews)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f8b09f1a390>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"classes = [ 'negative' , 'positive']\n\n\n#y = movie_reviews[\"sentiment\"]\ndf = pd.concat([movie_reviews,pd.get_dummies(movie_reviews['sentiment'])],axis=1)\n#y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))\nx_train = movie_reviews[\"review\"]"},{"metadata":{},"cell_type":"markdown","source":"#print(y[1:3])\nclasses = [ 'negative' , 'positive']\ny = df[classes].values\ny"},{"metadata":{"trusted":true},"cell_type":"code","source":"#configuration  parameters\nLATENT_DIM_DECODER = 400\nBATCH_SIZE =128\nEPOCHS = 20\nLATENT_DIM = 400\nNUM_SAMPLES = 50000\nMAX_SEQUENCE_LEN = 1000\nMAX_NUM_WORDS = 50000\nEMBEDDING_DIM = 300","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#NLTK python library for preprocessing\nimport nltk\n#nltk.download('wordnet')\n#for tokenization\nfrom nltk.tokenize import RegexpTokenizer\n#for stemming\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\n#for removing stopwords\nfrom nltk.corpus import stopwords\n#importing regex library of python\nimport re\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer() \n#function for performing all preproccing steps at once\ndef preprocess(sentence):\n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence=sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    rem_url=re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens]#  if not w in stopwords.words('english')]\n    stem_words=[stemmer.stem(w) for w in filtered_words]\n    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n    return \" \".join(lemma_words)\ndef clean(tweet):\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    \n    #correct some acronyms while we are at it\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)      \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet) \n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)  \n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)  \n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    \n    #and some typos/abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    #hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n\n    \n    return tweet\n\n#make a dataframe of preprocessed text\ndf['cleanText']=x_train.map(lambda s:preprocess(s))\ndf['cleanText'].apply(clean)"},{"metadata":{},"cell_type":"markdown","source":"train_final = df['cleanText']\nmax_feat=50000\n#tokenize sentence into list of words\ntokenizer = Tokenizer(num_words=max_feat)#setting up tokenizer\n#fiiting the tokenizer on out data\ntokenizer.fit_on_texts(list(train_final))"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sequence_train = tokenizer.texts_to_sequences(train_final)\n#sequence_test = tokenizer2.texts_to_sequences(test_final)\n# get the word to index mapping for input language\n#word2idx_inputs = tokenizer.word_index\n#print('Found %s unique input tokens.' % len(word2idx_inputs))","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max_len = [len(s) for s in sequence_train]\n#print(max(max_len))","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dimension of input to the layer should be constant\n#scaling each comment sequence to a fixed length to 200\n#comments smaller than 200 will be padded with zeros to make their length as 200\nmax_len=800\n#pad the train and text sequence to be of fixed length (in keras input in lstm should be of fixed length sequnece)\nX_train = pad_sequences(X_train, padding='post', maxlen=max_len)\nX_test = pad_sequences(X_test, padding='post', maxlen=max_len)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input,SpatialDropout1D,Conv1D,LSTM,GlobalMaxPool1D,Embedding,Dropout,Bidirectional,GlobalMaxPool1D\nfrom keras.models import Sequential\nmodel = Sequential()\n#input_ = Input(shape=(max_len,))","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_layer = Embedding(vocab_size,300,input_length = 800,mask_zero = True)\nmodel.add(embed_layer)\nmodel.add(SpatialDropout1D(0.4))","execution_count":19,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-89586022f6f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membed_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpatialDropout1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    196\u001b[0m           \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m           \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m           \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m           \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/embeddings.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# right now. Checking for the presence of GPUs to avoid complicating the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# TPU codepaths which can handle sparse optimizers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         self.embeddings = self.add_weight(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mnum_gpus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1045\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnum_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;34m\"\"\"The number of GPUs available to execute operations.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_gpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m           pywrap_tfe.TFE_ContextOptionsSetLazyRemoteInputsCopy(\n\u001b[1;32m    514\u001b[0m               opts, self._lazy_remote_inputs_copy)\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mcontext_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_NewContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_DeleteContextOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn = Conv1D(128,3)\nLSTM_layer =LSTM(64, return_sequences = True,name='rnn_layer',recurrent_dropout = 0.4)\n#LSTM_layer =LSTM(128, return_sequences = True,name='rnn_layer')\n#sec_LSTM_layer = Bidirectional(LSTM(256, return_sequences=True, name='BI2_lstm_layer'))(LSTM_layer)\nmodel.add(cnn)\nmodel.add(LSTM_layer)\n\nmodel.add( Dropout(0.55))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dimension reduction using pooling layer\nfrom keras.layers import GlobalAvgPool1D\nmodel.add(GlobalMaxPool1D())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dropout(0.55))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(128,activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dropout(0.55))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Dense(1,activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer = Adam(lr=0.001),\n             metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.optimizers import Adagrad,Adam,RMSprop,Adamax\n#model = Model(inputs=input_ , outputs = output_dense)\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=3, verbose=1, validation_split=0.3)\n\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('/kaggle/working/bin_sentiment_try3.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import simplejson as json","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_json = tokenizer.to_json()\nwith open('tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = load_model('/kaggle/working/bin_sentiment_try3.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import tokenizer_from_json\nimport simplejson as json\nwith open('/kaggle/working/tokenizer.json') as f:\n    data = json.load(f)\n    tokenizer_load = tokenizer_from_json(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\ns = ['i did not liked it, it was not good']\nsequences_custom = tokenizer_load.texts_to_sequences(s)\npadded_seq_cus = pad_sequences(sequences_custom,maxlen=1000,padding  = 'post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = model2.predict(padded_seq_cus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if x >0.5:\n    print(\"positive\")\nelse:\n    print(\"negative\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(padded_seq_cus.shape)\nprint(type(padded_seq_cus))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}