{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\anaconda_installation_files\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#importing all the libraries\n",
    "\n",
    "import sys , os , re , csv , codecs , numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Input,LSTM,Embedding,Dropout,Activation\n",
    "from keras.layers import Bidirectional,GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints , optimizers, layers\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the train data\n",
    "train = pd.read_csv('train.csv')\n",
    "#loading the test data\n",
    "test = pd.read_csv('test.csv')\n",
    "#diplay first 5 rows of train\n",
    "train.head()\n",
    "#one hot encoding the labels\n",
    "df = pd.concat([train,pd.get_dummies(train['sentiment'])],axis=1)\n",
    "#df.head()\n",
    "train_data = df['tweet_text']\n",
    "#train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.concat([test,pd.get_dummies(test['sentiment'])],axis=1)\n",
    "#classes2 = ['neutral' , 'negative' , 'positive']\n",
    "#y_val = df_test[classes2].values\n",
    "#y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @jjuueellzz down in the Atlantic city, ventnor...\n",
       "1    Musical awareness: Great Big Beautiful Tomorro...\n",
       "2    On Radio786 100.4fm 7:10 Fri Oct 19 Labour ana...\n",
       "3    Kapan sih lo ngebuktiin,jan ngomong doang Susa...\n",
       "4    Excuse the connectivity of this live stream, f...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test['tweet_text']\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the array of labels in serial with their respective texts\n",
    "classes = ['neutral' , 'negative' , 'positive']\n",
    "y = df[classes].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id      False\n",
       "tweet_text    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values in train and test data\n",
    "train.isnull().any()\n",
    "test.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration  parameters\n",
    "LATENT_DIM_DECODER = 400\n",
    "BATCH_SIZE =32\n",
    "EPOCHS = 10\n",
    "LATENT_DIM = 400\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_SEQUENCE_LEN = 1000\n",
    "MAX_NUM_WORDS = 40000\n",
    "EMBEDDING_DIM = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK python library for preprocessing\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "#for tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#for stemming\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "#for removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "#importing regex library of python\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "#function for performing all preproccing steps at once\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens]#  if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "#make a dataframe of preprocessed text\n",
    "df['cleanText']=train_data.map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       jjuueellzz down in the atlantic city ventnor m...\n",
       "1       musical awareness great big beautiful tomorrow...\n",
       "2       on radio fm fri oct labour analyst shawn hatti...\n",
       "3       kapan sih lo ngebuktiin jan ngomong doang susa...\n",
       "4       excuse the connectivity of this live stream fr...\n",
       "                              ...                        \n",
       "5393    it s a wednesday girls night out as s band wil...\n",
       "5394    night college course sorted just have to enrol...\n",
       "5395    for the st time in years for your splendiferou...\n",
       "5396    nurses day may nursing the heart beat of the h...\n",
       "5397    we have minutes left until the nd episode of s...\n",
       "Name: clean_text, Length: 5398, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['clean_text']=test['tweet_text'].map(lambda s:preprocess(s))\n",
    "test_final = test['clean_text']\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking the sentence into unique words/tokens\n",
    "#expecting max tokens to be 20k\n",
    "train_final = df['cleanText']\n",
    "max_feat=40000\n",
    "#tokenize sentence into list of words\n",
    "tokenizer = Tokenizer(num_words=max_feat)#setting up tokenizer\n",
    "#fiiting the tokenizer on out data\n",
    "tokenizer.fit_on_texts(list(train_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        gas by my house hit i um going to chapel hill ...\n",
       "1        theo walcott is still shit uc watch rafa and j...\n",
       "2        its not that i um a gsp fan uc i just hate nic...\n",
       "3        iranian general says israel us iron dome can u...\n",
       "4        tehran uc mon amour obama tried to establish t...\n",
       "                               ...                        \n",
       "21460    the day after newark ill be able to say i met ...\n",
       "21461    fec hold farewell session for seven ministers ...\n",
       "21462    luca di montezemolo who s last day was monday ...\n",
       "21463    coffee is pretty much the answer to all questi...\n",
       "21464    niki lauda just confirmed to sky that alonso w...\n",
       "Name: cleanText, Length: 21465, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(num_words=max_feat)#setting up tokenizer\n",
    "#fiiting the tokenizer on out data\n",
    "tokenizer2.fit_on_texts(list(test_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34302 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "#converting text into sequence of numbers to feed in neural network\n",
    "sequence_train = tokenizer.texts_to_sequences(train_final)\n",
    "sequence_test = tokenizer2.texts_to_sequences(test_final)\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#LOADING PRETRAINED WORD VECTORS\n",
    "# store all the pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open('glove.6B.100d.txt', encoding=\"utf8\") as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "#EMBEDDING MATRIX\n",
    "# prepare embedding matrix of words for embedding layer\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "  if i < MAX_NUM_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1074\n"
     ]
    }
   ],
   "source": [
    "max_len = [len(s) for s in sequence_train]\n",
    "print(max(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling all the sequences to a fixed length\n",
    "#dimension of input to the layer should be constant\n",
    "#scaling each comment sequence to a fixed length to 200\n",
    "#comments smaller than 200 will be padded with zeros to make their length as 200\n",
    "max_len=1000\n",
    "#pad the train and text sequence to be of fixed length (in keras input in lstm should be of fixed length sequnece)\n",
    "x_train=pad_sequences(sequence_train,maxlen=max_len)\n",
    "x_test=pad_sequences(sequence_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len,\n",
    "  trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if we used right length of sequence padding \n",
    "for greater length of comments we have to take such length sequence padding that on deleting some information doesnt result in loss of important imformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_words = [len(words) for words in sequence_train]\n",
    "#distribution of sequence\n",
    "plt.hist(len_words, bins = np.arange(0,400,10))\n",
    "plt.show()\n",
    "# we can see that most of the comments have [0,50]  words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training data is ready to be fed into the input layer as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feeding the output of previous layer to the embedding layer that converts \n",
    "#the sequences into vector representation to detect relevance and context \n",
    "#of a particular word\n",
    "embed_layer =embedding_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the previous output as input to the BI_LSTM layer\n",
    "LSTM_layer = Bidirectional(LSTM(128, return_sequences=True, name='BI_lstm_layer'))(embed_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimension reduction using pooling layer\n",
    "red_dim_layer = GlobalMaxPool1D()(LSTM_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### adding dropout layer for better generalization\n",
    "#setting value as 0.1 , which means 10$ of nodes will be randomly disabled\n",
    "drop_layer = Dropout(0.5)(red_dim_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#densely connected layer\n",
    "dense1 = Dense(128,activation='relu')(drop_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding another dropout layer\n",
    "drop_layer2 = Dropout(0.5)(dense1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the output dense layer with sigmoid activation to get result \n",
    "#3  classes as output\n",
    "output_dense = Dense(3,activation='softmax')(drop_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         3430300   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 256)         234496    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 3,698,079\n",
      "Trainable params: 267,779\n",
      "Non-trainable params: 3,430,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#connecting the inputs and outputs to create a model and compiling the model\n",
    "from keras.optimizers import Adagrad,Adam\n",
    "model = Model(inputs=input , outputs = output_dense)\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = Adam(lr=0.001),\n",
    "             metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anaconda_installation_files\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 17172 samples, validate on 4293 samples\n",
      "Epoch 1/20\n",
      "17172/17172 [==============================] - 1022s 60ms/step - loss: 0.9472 - accuracy: 0.5217 - val_loss: 0.8524 - val_accuracy: 0.5779\n",
      "Epoch 2/20\n",
      "17172/17172 [==============================] - 1013s 59ms/step - loss: 0.8311 - accuracy: 0.6120 - val_loss: 0.8503 - val_accuracy: 0.5889\n",
      "Epoch 3/20\n",
      "17172/17172 [==============================] - 1034s 60ms/step - loss: 0.7863 - accuracy: 0.6396 - val_loss: 0.8282 - val_accuracy: 0.5968\n",
      "Epoch 4/20\n",
      "17172/17172 [==============================] - 1074s 63ms/step - loss: 0.7608 - accuracy: 0.6540 - val_loss: 0.8286 - val_accuracy: 0.6061\n",
      "Epoch 5/20\n",
      "17172/17172 [==============================] - 1048s 61ms/step - loss: 0.7333 - accuracy: 0.6654 - val_loss: 0.8468 - val_accuracy: 0.5996\n",
      "Epoch 6/20\n",
      "17172/17172 [==============================] - 1078s 63ms/step - loss: 0.7069 - accuracy: 0.6824 - val_loss: 0.8261 - val_accuracy: 0.6108\n",
      "Epoch 7/20\n",
      "17172/17172 [==============================] - 1036s 60ms/step - loss: 0.6864 - accuracy: 0.6942 - val_loss: 0.8556 - val_accuracy: 0.5993\n",
      "Epoch 8/20\n",
      "17172/17172 [==============================] - 1020s 59ms/step - loss: 0.6629 - accuracy: 0.7052 - val_loss: 0.8698 - val_accuracy: 0.5826\n",
      "Epoch 9/20\n",
      "17172/17172 [==============================] - 1070s 62ms/step - loss: 0.6397 - accuracy: 0.7172 - val_loss: 0.8486 - val_accuracy: 0.6045\n",
      "Epoch 10/20\n",
      "17172/17172 [==============================] - 1033s 60ms/step - loss: 0.6176 - accuracy: 0.7268 - val_loss: 0.9074 - val_accuracy: 0.5954\n",
      "Epoch 11/20\n",
      "17172/17172 [==============================] - 1004s 58ms/step - loss: 0.5999 - accuracy: 0.7371 - val_loss: 0.8713 - val_accuracy: 0.6063\n",
      "Epoch 12/20\n",
      "17172/17172 [==============================] - 1026s 60ms/step - loss: 0.5678 - accuracy: 0.7529 - val_loss: 0.9042 - val_accuracy: 0.6101\n",
      "Epoch 13/20\n",
      "17172/17172 [==============================] - 1012s 59ms/step - loss: 0.5561 - accuracy: 0.7545 - val_loss: 0.9000 - val_accuracy: 0.5884\n",
      "Epoch 14/20\n",
      "17172/17172 [==============================] - 1022s 60ms/step - loss: 0.5346 - accuracy: 0.7643 - val_loss: 0.9477 - val_accuracy: 0.5949\n",
      "Epoch 15/20\n",
      "17172/17172 [==============================] - 1010s 59ms/step - loss: 0.5053 - accuracy: 0.7859 - val_loss: 1.0201 - val_accuracy: 0.5782\n",
      "Epoch 16/20\n",
      "17172/17172 [==============================] - 1014s 59ms/step - loss: 0.4914 - accuracy: 0.7904 - val_loss: 0.9973 - val_accuracy: 0.6049\n",
      "Epoch 17/20\n",
      "17172/17172 [==============================] - 1000s 58ms/step - loss: 0.4659 - accuracy: 0.8015 - val_loss: 1.1117 - val_accuracy: 0.5861\n",
      "Epoch 18/20\n",
      "17172/17172 [==============================] - 1004s 58ms/step - loss: 0.4638 - accuracy: 0.8042 - val_loss: 1.0586 - val_accuracy: 0.5933\n",
      "Epoch 19/20\n",
      "17172/17172 [==============================] - 991s 58ms/step - loss: 0.4369 - accuracy: 0.8152 - val_loss: 1.0900 - val_accuracy: 0.5949\n",
      "Epoch 20/20\n",
      "17172/17172 [==============================] - 998s 58ms/step - loss: 0.4259 - accuracy: 0.8163 - val_loss: 1.1403 - val_accuracy: 0.5903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1944ee9e808>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model \n",
    "batch_size=32\n",
    "epochs = 20\n",
    "model.fit(x_train,y,batch_size=batch_size,epochs = epochs,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment2iit2_glove_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
